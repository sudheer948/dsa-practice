# ðŸ“˜ Day 3B â€” Common Time Complexities & Growth Order

---

## ðŸŸ¢ O(1) â€” Constant Time

The algorithm takes the same time regardless of input size.

```javascript
return arr[5];
```

Even if array size = 10 or 1,000,000 â†’ time stays constant.

---

## ðŸŸ¢ O(log n) â€” Logarithmic Time

Problem size is divided by 2 repeatedly.

Example:
- Binary Search

Growth pattern:

```
n â†’ n/2 â†’ n/4 â†’ n/8 â†’ ...
```

Very efficient for large inputs.

---

## ðŸŸ¢ O(n) â€” Linear Time

Single loop.

```javascript
for (let i = 0; i < n; i++) {
    console.log(i);
}
```

Operations grow directly with `n`.

---

## ðŸŸ¢ O(n log n)

Combination of linear and logarithmic behavior.

Common examples:
- Merge Sort
- Quick Sort (average case)

---

## ðŸ”´ O(nÂ²) â€” Quadratic Time

Nested loops.

```javascript
for (let i = 0; i < n; i++) {
    for (let j = 0; j < n; j++) {
        console.log(i, j);
    }
}
```

Operations = n Ã— n = nÂ²

---

## ðŸ”´ O(2â¿) â€” Exponential Time

Growth doubles as `n` increases.

Common in brute-force recursion.

---

## ðŸ”´ O(n!) â€” Factorial Time

Seen in permutation problems.

Extremely slow for large `n`.

---

## ðŸ“ˆ Growth Order (Best â†’ Worst)

```
O(1) < O(log n) < O(n) < O(n log n) < O(nÂ²) < O(2â¿) < O(n!)
```

Left side = More efficient  
Right side = Less efficient  

As `n` becomes large, differences become massive.

---

## ðŸŽ¯ Key Takeaways (Day 3B)

- O(1) is most efficient.
- Nested loops often lead to O(nÂ²).
- Exponential and factorial complexities grow very fast.
- Always try to move left in growth order.